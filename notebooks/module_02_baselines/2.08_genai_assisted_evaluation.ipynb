{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.08: GenAI-Assisted Performance Evaluation\n",
    "\n",
    "## OPENING: YOU EARNED THE RIGHT TO USE GENAI\n",
    "\n",
    "We've built the baseline system. We've run cross-validation. We've computed metrics. We've built a scoreboard. We've run portfolio diagnostics. We've done forensic investigation.\n",
    "\n",
    "**Now we can use GenAI.**\n",
    "\n",
    "Here's why the order matters: In early forecasting work, GenAI is dangerous. You don't have grounding. You don't have evidence.\n",
    "\n",
    "But now? **Now we have grounding.**\n",
    "\n",
    "We have the forecast database from cross-validation. We have the scoreboard. We have diagnostics. We have forensic findings. We have artifacts that represent measured truth.\n",
    "\n",
    "GenAI doesn't create truth. **It compresses truth you already measured.**\n",
    "\n",
    "**Rule: If GenAI contradicts the scoreboard, GenAI loses.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP: Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# For GenAI integration (example using OpenAI or similar)\n",
    "# pip install openai\n",
    "# from openai import OpenAI\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evidence artifacts from previous modules\n",
    "scoreboard = pd.read_csv('path/to/scoreboard.csv')\n",
    "diagnostics_summary = pd.read_csv('path/to/diagnostics_summary.csv')\n",
    "forensic_findings = pd.read_csv('forensic_findings_for_module_3.csv')\n",
    "\n",
    "print(f\"Loaded scoreboard with {len(scoreboard)} models\")\n",
    "print(f\"Loaded diagnostics summary with {len(diagnostics_summary)} findings\")\n",
    "print(f\"Loaded forensic findings with {len(forensic_findings)} SKUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 1: THE GENAI GOVERNANCE CONTRACT\n",
    "\n",
    "### Four Rules (The Guardrails)\n",
    "\n",
    "Before we start prompting GenAI, we need rules. These aren't suggestions — they're hard boundaries.\n",
    "\n",
    "**Rule 1: Evidence In, Insight Out**\n",
    "- GenAI only interprets data we measured\n",
    "- No speculation about unmeasured scenarios\n",
    "\n",
    "**Rule 2: Scoreboard Veto**\n",
    "- If GenAI's interpretation contradicts the metrics, the metrics win\n",
    "- GenAI can suggest *why* a pattern exists, but can't deny that it exists\n",
    "\n",
    "**Rule 3: Chain-of-Custody for Insights**\n",
    "- Every GenAI output must be traceable back to a data artifact\n",
    "- \"Model X is better because...\" must cite the scoreboard\n",
    "\n",
    "**Rule 4: Human Authority**\n",
    "- GenAI generates hypotheses, not decisions\n",
    "- The human makes the final call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 2: PROMPT ENGINEERING FOR FORECASTING DIAGNOSTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_genai_context_prompt(scoreboard_df, diagnostics_df, forensic_df):\n",
    "    \"\"\"\n",
    "    Create a grounded context prompt for GenAI analysis\n",
    "    Follows Rule 1 & 3: Evidence In, Chain-of-Custody\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build evidence summary\n",
    "    scoreboard_summary = f\"\"\"\n",
    "    Scoreboard Summary (Evidence):\n",
    "    - Best model: {scoreboard_df.iloc[0]['model_name'] if len(scoreboard_df) > 0 else 'N/A'}\n",
    "    - Best wMAPE: {scoreboard_df.iloc[0]['wmape']:.2%} if len(scoreboard_df) > 0 else 'N/A'}\n",
    "    - Model count: {len(scoreboard_df)}\n",
    "    \"\"\"\n",
    "    \n",
    "    diagnostics_summary = f\"\"\"\n",
    "    Diagnostics Summary (Evidence):\n",
    "    - High-error segments identified: {diagnostics_df['segment'].nunique() if len(diagnostics_df) > 0 else 'N/A'}\n",
    "    - Primary failure mode: {diagnostics_df.iloc[0]['action'] if len(diagnostics_df) > 0 else 'N/A'}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are analyzing forecasting model performance. Your role is to:\n",
    "    1. Interpret measured evidence (scoreboard, diagnostics, forensics)\n",
    "    2. Identify patterns that suggest specific fixes\n",
    "    3. Generate hypotheses about what features or techniques could improve performance\n",
    "    \n",
    "    CRITICAL: Base all interpretations on the evidence below. If evidence contradicts your interpretation, the evidence wins.\n",
    "    \n",
    "    {scoreboard_summary}\n",
    "    \n",
    "    {diagnostics_summary}\n",
    "    \n",
    "    Given this evidence, please:\n",
    "    1. Summarize the current model performance status\n",
    "    2. Identify the top 3 actionable insights for improvement\n",
    "    3. For each insight, suggest specific features or techniques\n",
    "    4. Prioritize by impact potential\n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Generate context prompt\n",
    "context_prompt = create_genai_context_prompt(scoreboard, diagnostics_summary, forensic_findings)\n",
    "print(\"Context Prompt Generated:\")\n",
    "print(context_prompt[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Mock GenAI response (in real scenario, call actual API)\n",
    "def mock_genai_analysis(context):\n",
    "    \"\"\"\n",
    "    Mock GenAI response for demonstration\n",
    "    In production, replace with actual API call to OpenAI/Claude/etc\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'status': 'Baseline system is performing adequately',\n",
    "        'top_insights': [\n",
    "            {\n",
    "                'insight': 'Seasonal patterns are not fully captured',\n",
    "                'evidence': 'Residual analysis shows 52-week autocorrelation',\n",
    "                'recommendation': 'Add explicit seasonal indicators (month, quarter, day-of-week)',\n",
    "                'impact': 'HIGH',\n",
    "                'effort': 'LOW'\n",
    "            },\n",
    "            {\n",
    "                'insight': 'Model misses promotional spikes',\n",
    "                'evidence': 'Forensic investigation identified holiday/promo alignment with errors',\n",
    "                'recommendation': 'Create promotional calendar feature and include in model',\n",
    "                'impact': 'MEDIUM',\n",
    "                'effort': 'MEDIUM'\n",
    "            },\n",
    "            {\n",
    "                'insight': 'Variance is high in long-tail items',\n",
    "                'evidence': 'Diagnostics show error concentration in low-volume SKUs',\n",
    "                'recommendation': 'Use ensemble approach or hybrid policy (forecast + inventory rule)',\n",
    "                'impact': 'MEDIUM',\n",
    "                'effort': 'HIGH'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return analysis\n",
    "\n",
    "# Run mock analysis\n",
    "genai_analysis = mock_genai_analysis(context_prompt)\n",
    "print(\"\\nGenAI Analysis Results:\")\n",
    "print(f\"Status: {genai_analysis['status']}\")\n",
    "print(f\"\\nTop Insights:\")\n",
    "for i, insight in enumerate(genai_analysis['top_insights'], 1):\n",
    "    print(f\"\\n{i}. {insight['insight']}\")\n",
    "    print(f\"   Evidence: {insight['evidence']}\")\n",
    "    print(f\"   Recommendation: {insight['recommendation']}\")\n",
    "    print(f\"   Impact: {insight['impact']} | Effort: {insight['effort']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 3: HYPOTHESIS GENERATION FROM EVIDENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_genai_hypothesis(hypothesis, scoreboard_df, tolerance=0.05):\n",
    "    \"\"\"\n",
    "    Validate GenAI hypothesis against measured evidence\n",
    "    Rule 2 & 3: Scoreboard Veto + Chain-of-Custody\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_result = {\n",
    "        'hypothesis': hypothesis,\n",
    "        'is_grounded': False,\n",
    "        'supporting_evidence': [],\n",
    "        'contradicting_evidence': [],\n",
    "        'confidence': 0.0\n",
    "    }\n",
    "    \n",
    "    # Example: Check if hypothesis about model performance is supported\n",
    "    if 'seasonal' in hypothesis.lower():\n",
    "        # Check if any model performs significantly better in high-seasonality items\n",
    "        if len(scoreboard_df) > 0:\n",
    "            validation_result['supporting_evidence'].append(\n",
    "                f\"Scoreboard shows {scoreboard_df.iloc[0]['model_name']} has wMAPE of {scoreboard_df.iloc[0]['wmape']:.2%}\"\n",
    "            )\n",
    "            validation_result['is_grounded'] = True\n",
    "            validation_result['confidence'] = 0.7\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "# Validate each GenAI hypothesis\n",
    "validated_hypotheses = []\n",
    "for insight in genai_analysis['top_insights']:\n",
    "    validation = validate_genai_hypothesis(insight['insight'], scoreboard)\n",
    "    validated_hypotheses.append(validation)\n",
    "\n",
    "print(\"\\nHypothesis Validation (Rule 2: Scoreboard Veto):\")\n",
    "for i, val in enumerate(validated_hypotheses, 1):\n",
    "    print(f\"\\n{i}. {val['hypothesis']}\")\n",
    "    print(f\"   Grounded in Evidence: {val['is_grounded']}\")\n",
    "    print(f\"   Confidence: {val['confidence']:.0%}\")\n",
    "    if val['supporting_evidence']:\n",
    "        print(f\"   Supporting Evidence: {val['supporting_evidence'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 4: ACTIONABLE BACKLOG GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_backlog_from_genai(genai_insights, validated_hypotheses):\n",
    "    \"\"\"\n",
    "    Convert GenAI insights + validated hypotheses into actionable backlog\n",
    "    Rule 4: GenAI generates hypotheses, humans prioritize\n",
    "    \"\"\"\n",
    "    \n",
    "    backlog = []\n",
    "    \n",
    "    for insight, validation in zip(genai_insights['top_insights'], validated_hypotheses):\n",
    "        if validation['is_grounded']:  # Only include grounded insights\n",
    "            backlog_item = {\n",
    "                'feature': insight['recommendation'],\n",
    "                'insight': insight['insight'],\n",
    "                'evidence': validation['supporting_evidence'],\n",
    "                'impact_potential': insight['impact'],\n",
    "                'implementation_effort': insight['effort'],\n",
    "                'priority_score': calculate_priority(insight['impact'], insight['effort']),\n",
    "                'status': 'PENDING_REVIEW',\n",
    "                'module': 'Module 3: Feature Engineering'\n",
    "            }\n",
    "            backlog.append(backlog_item)\n",
    "    \n",
    "    return pd.DataFrame(backlog)\n",
    "\n",
    "def calculate_priority(impact, effort):\n",
    "    \"\"\"\n",
    "    Simple priority score: impact / effort\n",
    "    HIGH=3, MEDIUM=2, LOW=1\n",
    "    \"\"\"\n",
    "    impact_val = {'HIGH': 3, 'MEDIUM': 2, 'LOW': 1}.get(impact, 1)\n",
    "    effort_val = {'LOW': 3, 'MEDIUM': 2, 'HIGH': 1}.get(effort, 1)\n",
    "    return impact_val / effort_val if effort_val > 0 else 0\n",
    "\n",
    "# Generate backlog\n",
    "feature_backlog = generate_feature_backlog_from_genai(genai_analysis, validated_hypotheses)\n",
    "feature_backlog = feature_backlog.sort_values('priority_score', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Engineering Backlog (Rule 4: Human Authority):\")\n",
    "print(feature_backlog[['feature', 'insight', 'impact_potential', 'implementation_effort', 'priority_score']].to_string(index=False))\n",
    "\n",
    "# Save backlog for Module 3\n",
    "feature_backlog.to_csv('genai_feature_backlog.csv', index=False)\n",
    "print(\"\\n✓ Feature backlog saved to 'genai_feature_backlog.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 5: WHEN GENAI CONTRADICTS EVIDENCE (RULE 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_scoreboard_veto(genai_claim, scoreboard_df):\n",
    "    \"\"\"\n",
    "    Check if GenAI claim contradicts scoreboard evidence\n",
    "    Rule 2: Scoreboard Veto\n",
    "    \"\"\"\n",
    "    \n",
    "    veto_result = {\n",
    "        'claim': genai_claim,\n",
    "        'contradicts_evidence': False,\n",
    "        'veto_reason': None,\n",
    "        'evidence_override': None\n",
    "    }\n",
    "    \n",
    "    # Example: If GenAI claims \"Model X is best\" but scoreboard shows otherwise\n",
    "    if 'best model' in genai_claim.lower():\n",
    "        if len(scoreboard_df) > 0:\n",
    "            actual_best = scoreboard_df.iloc[0]['model_name']\n",
    "            if actual_best not in genai_claim:\n",
    "                veto_result['contradicts_evidence'] = True\n",
    "                veto_result['veto_reason'] = f\"Scoreboard shows {actual_best} is actually the best\"\n",
    "                veto_result['evidence_override'] = f\"Use {actual_best} instead\"\n",
    "    \n",
    "    return veto_result\n",
    "\n",
    "# Example veto check\n",
    "example_claim = \"Model Theta seems like the best choice\"\n",
    "veto = check_scoreboard_veto(example_claim, scoreboard)\n",
    "\n",
    "print(f\"Claim: {veto['claim']}\")\n",
    "print(f\"Contradicts Evidence: {veto['contradicts_evidence']}\")\n",
    "if veto['contradicts_evidence']:\n",
    "    print(f\"Veto: {veto['veto_reason']}\")\n",
    "    print(f\"Evidence Override: {veto['evidence_override']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 6: DELIVERABLE - EXECUTIVE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary combining evidence + GenAI interpretation\n",
    "executive_summary = f\"\"\"\n",
    "FORECASTING SYSTEM EVALUATION SUMMARY\n",
    "{'='*70}\n",
    "\n",
    "CURRENT STATE (Evidence-Based)\n",
    "- Champion Model: {scoreboard.iloc[0]['model_name'] if len(scoreboard) > 0 else 'N/A'}\n",
    "- Portfolio wMAPE: {scoreboard.iloc[0]['wmape']:.2%} if len(scoreboard) > 0 else 'N/A'}\n",
    "- Assessment Status: Ready for diagnostics review\n",
    "\n",
    "KEY FINDINGS (Grounded Analysis)\n",
    "\"\"\"\n",
    "\n",
    "for i, insight in enumerate(genai_analysis['top_insights'][:3], 1):\n",
    "    executive_summary += f\"\"\"\n",
    "\n{i}. {insight['insight']}\n   Recommendation: {insight['recommendation']}\n   Priority: {insight['impact']} Impact / {insight['effort']} Effort\n\"\"\"\n",
    "\n",
    "executive_summary += f\"\"\"\n\nNEXT STEPS (Module 3)\n- Review recommended features\n- Implement highest priority items\n- Validate improvements via cross-validation\n\nGOVERNANCE NOTES\n- All GenAI recommendations grounded in measured evidence\n- Scoreboard remains final authority for model selection\n- Feature priorities should be reviewed by human decision-maker\n{'='*70}\n\"\"\"\n",
    "\n",
    "print(executive_summary)\n",
    "\n",
    "# Save summary\n",
    "with open('executive_summary_2.08.txt', 'w') as f:\n",
    "    f.write(executive_summary)\n",
    "\n",
    "print(\"\\n✓ Executive summary saved to 'executive_summary_2.08.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
