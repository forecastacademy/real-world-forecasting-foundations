{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.06: Model Performance Diagnostics - Executive Level\n",
    "\n",
    "## OPENING\n",
    "\n",
    "Alright — we've computed metrics, we've built the scoreboard, and we've compared our baselines.\n",
    "\n",
    "But here's the thing: **No model is signed off yet.**\n",
    "\n",
    "Before we deploy anything — before we call a baseline \"good enough\" — we need due diligence. This is inspection before deployment.\n",
    "\n",
    "Even if a baseline passes the Sufficiency Gate, diagnostics are required for risk management.\n",
    "\n",
    "Executives don't want 30,000 individual SKU metrics. They want a system health report. They want to know: **Where is the fire?**\n",
    "\n",
    "This notebook takes us from scoreboard rankings to diagnostic triage — the repeatable workflow real forecasting teams use to assess health at the portfolio level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP: Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme()\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CV results from Module 2.05\n",
    "# This should contain cross-validation metrics for each model across SKUs\n",
    "cv_results = pd.read_csv('path/to/cv_results.csv')\n",
    "sku_metadata = pd.read_csv('path/to/sku_metadata.csv')\n",
    "\n",
    "print(f\"Loaded {len(cv_results)} cross-validation records\")\n",
    "print(f\"Loaded {len(sku_metadata)} SKU metadata records\")\n",
    "print(f\"\\nCV Results columns: {cv_results.columns.tolist()}\")\n",
    "print(f\"\\nSKU Metadata columns: {sku_metadata.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 1: THE DIAGNOSTIC MINDSET\n",
    "\n",
    "### Diagnostics Are a Roadmap, Not a Ranking\n",
    "\n",
    "The scoreboard told us who scored best. But that doesn't tell us what to do next.\n",
    "\n",
    "Diagnostics answer different questions:\n",
    "- **Where** does the error live? Segments? Volume tiers?\n",
    "- **Does performance decay by horizon?** Short-term vs mid-term?\n",
    "- **Is the error systematic or random?** Bias vs variance?\n",
    "- **Is the model stable over time?** Consistent or volatile?\n",
    "- **Is the problem even forecastable?** Modeling failure or reality?\n",
    "\n",
    "### The 5-Question Triage Checklist\n",
    "\n",
    "**1. Where?** Find the 20% of segments causing 80% of the pain.\n",
    "\n",
    "**2. When?** Check for cliffs in horizon accuracy.\n",
    "\n",
    "**3. What Type?** Bias (fixable) vs Variance (manageable).\n",
    "\n",
    "**4. How Stable?** Check for planner-killing volatility.\n",
    "\n",
    "**5. Why?** Is this a modeling problem or a noise floor problem?\n",
    "\n",
    "**Pro tip:** Each section below answers one of these five questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 2: THE PARETO OF ERROR (WHERE THE FIRE IS)\n",
    "\n",
    "### Find the High-Impact Failure Zones Fast\n",
    "\n",
    "The first question: **Where does the error live?**\n",
    "\n",
    "We slice total portfolio error by three groups:\n",
    "- **Category:** Does most error come from Produce? Frozen?\n",
    "- **Volume Tier:** Are we failing on A-items (high impact) or C-items (noise)?\n",
    "- **Forecastability Quadrants:** Does the model fail where it should succeed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge CV results with metadata to get segment information\n",
    "diagnostics_df = cv_results.merge(sku_metadata, on='sku_id', how='left')\n",
    "\n",
    "# Calculate total portfolio error (using wMAPE)\n",
    "total_portfolio_wmape = diagnostics_df['wmape'].mean()\n",
    "print(f\"Total Portfolio wMAPE: {total_portfolio_wmape:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS 1: Error by Category\n",
    "error_by_category = diagnostics_df.groupby('category').agg({\n",
    "    'wmape': ['mean', 'count'],\n",
    "    'sku_id': 'count'\n",
    "}).round(4)\n",
    "\n",
    "error_by_category.columns = ['avg_wmape', 'num_records', 'num_skus']\n",
    "error_by_category['pct_of_portfolio'] = (error_by_category['num_records'] / error_by_category['num_records'].sum() * 100).round(1)\n",
    "error_by_category = error_by_category.sort_values('avg_wmape', ascending=False)\n",
    "\n",
    "print(\"Error Contribution by Category:\")\n",
    "print(error_by_category)\n",
    "print(f\"\\nTop contributor: {error_by_category.index[0]} with {error_by_category.iloc[0]['avg_wmape']:.2%} wMAPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS 2: Error by Volume Tier (A, B, C items)\n",
    "# Assuming 'volume_tier' column exists in metadata\n",
    "if 'volume_tier' in diagnostics_df.columns:\n",
    "    error_by_tier = diagnostics_df.groupby('volume_tier').agg({\n",
    "        'wmape': ['mean', 'count'],\n",
    "        'sku_id': 'nunique'\n",
    "    }).round(4)\n",
    "    \n",
    "    error_by_tier.columns = ['avg_wmape', 'num_records', 'num_skus']\n",
    "    error_by_tier['pct_of_portfolio'] = (error_by_tier['num_records'] / error_by_tier['num_records'].sum() * 100).round(1)\n",
    "    error_by_tier = error_by_tier.sort_values('avg_wmape', ascending=False)\n",
    "    \n",
    "    print(\"\\nError Contribution by Volume Tier:\")\n",
    "    print(error_by_tier)\n",
    "else:\n",
    "    print(\"\\nVolume tier column not found. Skipping this analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 3: BIAS VS VARIANCE (THE TWO FAILURE MODES)\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "**\"Bias tells you what to build. Variance tells you what to buffer.\"**\n",
    "\n",
    "- **Bias (Systematic Error):** Consistently high or low. Missing structure — a promo, a holiday, a trend. *Fixable via features.*\n",
    "- **Variance (Random Error):** Forecast jumps around wildly. Weak signal or noisy world. *Managed via smoothing or inventory buffers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Bias: Mean Percentage Error (MPE) - directional error\n",
    "# Assuming 'forecast' and 'actual' columns exist\n",
    "if 'forecast' in diagnostics_df.columns and 'actual' in diagnostics_df.columns:\n",
    "    diagnostics_df['mpe'] = ((diagnostics_df['forecast'] - diagnostics_df['actual']) / diagnostics_df['actual']).abs()\n",
    "    diagnostics_df['signed_pct_error'] = (diagnostics_df['forecast'] - diagnostics_df['actual']) / diagnostics_df['actual']\n",
    "    \n",
    "    # Bias = Mean Signed Percentage Error\n",
    "    diagnostics_df['bias'] = diagnostics_df['signed_pct_error']\n",
    "    \n",
    "    # Variance = Standard deviation of errors\n",
    "    diagnostics_df['error_magnitude'] = np.abs(diagnostics_df['forecast'] - diagnostics_df['actual'])\n",
    "    \n",
    "    # Segment analysis\n",
    "    bias_variance_by_category = diagnostics_df.groupby('category').agg({\n",
    "        'bias': 'mean',\n",
    "        'error_magnitude': 'std',\n",
    "        'wmape': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    bias_variance_by_category.columns = ['mean_bias', 'error_variance', 'wmape']\n",
    "    bias_variance_by_category = bias_variance_by_category.sort_values('wmape', ascending=False)\n",
    "    \n",
    "    print(\"Bias vs Variance by Category:\")\n",
    "    print(bias_variance_by_category)\n",
    "else:\n",
    "    print(\"Forecast and actual columns not found. Cannot compute bias/variance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify segments into action categories\n",
    "if 'bias' in diagnostics_df.columns:\n",
    "    by_category_summary = diagnostics_df.groupby('category').agg({\n",
    "        'bias': lambda x: x.abs().mean(),\n",
    "        'error_magnitude': 'std',\n",
    "        'wmape': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    by_category_summary.columns = ['avg_bias', 'error_volatility', 'wmape']\n",
    "    \n",
    "    # Classification\n",
    "    by_category_summary['action'] = 'Investigate'\n",
    "    high_bias = by_category_summary['avg_bias'] > by_category_summary['avg_bias'].median()\n",
    "    high_variance = by_category_summary['error_volatility'] > by_category_summary['error_volatility'].median()\n",
    "    \n",
    "    by_category_summary.loc[high_bias & ~high_variance, 'action'] = 'Feature Engineering'\n",
    "    by_category_summary.loc[~high_bias & high_variance, 'action'] = 'Buffer/Policy'\n",
    "    by_category_summary.loc[high_bias & high_variance, 'action'] = 'Both'\n",
    "    \n",
    "    print(\"\\nAction Plan by Segment:\")\n",
    "    print(by_category_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 4: STABILITY ACROSS ORIGINS (PLANNER PAIN)\n",
    "\n",
    "### The Fourth Question: Is the Model Stable Over Time?\n",
    "\n",
    "Planners hate volatility. A model that's consistently 12% accurate is better than one that swings between 8% and 18% every week.\n",
    "\n",
    "We measure stability using **standard deviation of wMAPE across cross-validation windows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'fold_id' or 'cv_window' column exists\n",
    "if 'fold_id' in cv_results.columns or 'cv_window' in cv_results.columns:\n",
    "    fold_col = 'fold_id' if 'fold_id' in cv_results.columns else 'cv_window'\n",
    "    \n",
    "    # Calculate wMAPE by fold\n",
    "    wmape_by_fold = cv_results.groupby(fold_col)['wmape'].mean()\n",
    "    \n",
    "    stability_metrics = {\n",
    "        'mean_wmape': wmape_by_fold.mean(),\n",
    "        'std_wmape': wmape_by_fold.std(),\n",
    "        'min_wmape': wmape_by_fold.min(),\n",
    "        'max_wmape': wmape_by_fold.max(),\n",
    "        'cv_coefficient': wmape_by_fold.std() / wmape_by_fold.mean()\n",
    "    }\n",
    "    \n",
    "    print(\"Model Stability Metrics:\")\n",
    "    for metric, value in stability_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    if stability_metrics['cv_coefficient'] > 0.1:\n",
    "        print(\"\\n⚠️  WARNING: High volatility detected. CV coefficient > 0.1\")\n",
    "    else:\n",
    "        print(\"\\n✓ Stability looks good. Model is consistent across folds.\")\n",
    "else:\n",
    "    print(\"Fold information not found. Skipping stability analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 5: HORIZON DIAGNOSTICS (SHORT VS MID-TERM)\n",
    "\n",
    "### The Second Question: Does Performance Decay by Horizon?\n",
    "\n",
    "Different horizons drive different decisions.\n",
    "- **Weeks 1-4 (Short Term):** Execution and replenishment\n",
    "- **Weeks 5-13 (Mid Term):** Ordering and planning\n",
    "\n",
    "If a model is great at week 1 but falls off a cliff at week 5, that matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'horizon' or 'forecast_horizon' column exists\n",
    "if 'horizon' in cv_results.columns or 'forecast_horizon' in cv_results.columns:\n",
    "    horizon_col = 'horizon' if 'horizon' in cv_results.columns else 'forecast_horizon'\n",
    "    \n",
    "    # Calculate wMAPE by horizon\n",
    "    wmape_by_horizon = cv_results.groupby(horizon_col)['wmape'].mean().sort_index()\n",
    "    \n",
    "    print(\"Error Curve by Forecast Horizon:\")\n",
    "    print(wmape_by_horizon)\n",
    "    \n",
    "    # Detect cliff\n",
    "    horizon_diffs = wmape_by_horizon.diff()\n",
    "    max_cliff = horizon_diffs.max()\n",
    "    max_cliff_horizon = horizon_diffs.idxmax()\n",
    "    \n",
    "    print(f\"\\nLargest accuracy drop: {max_cliff:.4f} at horizon {max_cliff_horizon}\")\n",
    "    \n",
    "    if max_cliff > 0.05:  # 5% drop\n",
    "        print(f\"⚠️  CLIFF DETECTED: Sharp performance drop at horizon {max_cliff_horizon}\")\n",
    "    else:\n",
    "        print(\"✓ Gradual decay pattern observed.\")\n",
    "else:\n",
    "    print(\"Horizon information not found. Skipping horizon diagnostics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize horizon decay if available\n",
    "if 'horizon' in cv_results.columns or 'forecast_horizon' in cv_results.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    wmape_by_horizon.plot(ax=ax, marker='o', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Forecast Horizon (weeks)')\n",
    "    ax.set_ylabel('wMAPE')\n",
    "    ax.set_title('Performance Decay by Horizon')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 6: FORECASTABILITY QUADRANTS (STRUCTURE × CHAOS)\n",
    "\n",
    "### Connecting Back to Module 1\n",
    "\n",
    "We map performance back to our Structure vs Chaos plane.\n",
    "\n",
    "- **Easy Wins (High Structure / Low Chaos):** Baseline should win. If it fails, something is broken.\n",
    "- **Variance Trap (High Structure / High Chaos):** Expect some jitter.\n",
    "- **Sparse Zone (Low Structure / Low Chaos):** Precision is impossible. Focus on bias.\n",
    "- **Unforecastable Core (Low Structure / High Chaos):** The baseline is expected to struggle here.\n",
    "\n",
    "Often we don't need \"a better model\" — we need **routing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'structure_score' and 'chaos_score' exist in metadata\n",
    "if 'structure_score' in sku_metadata.columns and 'chaos_score' in sku_metadata.columns:\n",
    "    quad_data = diagnostics_df.copy()\n",
    "    \n",
    "    # Define quadrants\n",
    "    structure_median = quad_data['structure_score'].median()\n",
    "    chaos_median = quad_data['chaos_score'].median()\n",
    "    \n",
    "    def assign_quadrant(row):\n",
    "        structure = row['structure_score'] >= structure_median\n",
    "        chaos = row['chaos_score'] >= chaos_median\n",
    "        \n",
    "        if structure and not chaos:\n",
    "            return 'Easy Wins'\n",
    "        elif structure and chaos:\n",
    "            return 'Variance Trap'\n",
    "        elif not structure and not chaos:\n",
    "            return 'Sparse Zone'\n",
    "        else:\n",
    "            return 'Unforecastable Core'\n",
    "    \n",
    "    quad_data['quadrant'] = quad_data.apply(assign_quadrant, axis=1)\n",
    "    \n",
    "    # Analyze by quadrant\n",
    "    quadrant_analysis = quad_data.groupby('quadrant').agg({\n",
    "        'wmape': ['mean', 'std', 'count'],\n",
    "        'sku_id': 'nunique'\n",
    "    }).round(4)\n",
    "    \n",
    "    quadrant_analysis.columns = ['avg_wmape', 'wmape_std', 'num_records', 'num_skus']\n",
    "    \n",
    "    print(\"Performance by Forecastability Quadrant:\")\n",
    "    print(quadrant_analysis)\n",
    "else:\n",
    "    print(\"Structure/Chaos scores not found in metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 7: SYSTEM HEALTH SUMMARY\n",
    "\n",
    "### Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate health summary report\n",
    "health_report = f\"\"\"\n",
    "SYSTEM HEALTH REPORT\n",
    "{'='*60}\n",
    "\n",
    "1. PORTFOLIO OVERVIEW\n",
    "   - Total wMAPE: {total_portfolio_wmape:.2%}\n",
    "   - Number of SKUs: {len(sku_metadata)}\n",
    "   - Number of Records: {len(cv_results)}\n",
    "\n",
    "2. KEY INSIGHTS\n",
    "\n",
    "   Where is the fire?\n",
    "   - Top error contributor: {error_by_category.index[0] if len(error_by_category) > 0 else 'N/A'}\n",
    "   - That segment contributes {error_by_category.iloc[0]['avg_wmape']:.2%} wMAPE\n",
    "   \n",
    "   What kind of error?\n",
    "   - Bias vs Variance analysis available in Section 3\n",
    "   \n",
    "   Is it stable?\n",
    "   - Check stability metrics in Section 4\n",
    "   \n",
    "   Performance by horizon?\n",
    "   - Check decay pattern in Section 5\n",
    "\n",
    "3. PRIORITY ACTIONS\n",
    "   - See action plan by segment (Section 3)\n",
    "   - Bias-driven segments → Feature Engineering\n",
    "   - Variance-driven segments → Buffering/Policy\n",
    "\n",
    "{'='*60}\n",
    "\"\"\"\n",
    "\n",
    "print(health_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 8: OUTPUTS AND HANDOFF\n",
    "\n",
    "### What We've Built\n",
    "\n",
    "We've answered our five diagnostic questions:\n",
    "1. ✓ **Where** - Pareto analysis\n",
    "2. ✓ **What Type** - Bias vs Variance\n",
    "3. ✓ **How Stable** - Stability metrics\n",
    "4. ✓ **When** - Horizon analysis\n",
    "5. ✓ **Why** - Forecastability quadrants\n",
    "\n",
    "### Deliverable: The System Health Report\n",
    "\n",
    "You now have:\n",
    "- **Top segments for feature engineering** (high bias)\n",
    "- **Top segments for inventory policy** (high variance)\n",
    "- **Horizon watchlist** (decay cliffs)\n",
    "- **Stability assessment** (planner trust factor)\n",
    "\n",
    "### What's Next\n",
    "\n",
    "We know *where* to focus. The next step is to know *why*.\n",
    "\n",
    "In Module 2.7, we go detective mode: visual inspection, SKU-level analysis, and root cause identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export priority backlog\n",
    "priority_segments = []\n",
    "\n",
    "# Add top segments by error\n",
    "top_error_segments = error_by_category.head(3).index.tolist()\n",
    "for segment in top_error_segments:\n",
    "    priority_segments.append({\n",
    "        'segment': segment,\n",
    "        'reason': 'High Error Contribution',\n",
    "        'priority': 'CRITICAL'\n",
    "    })\n",
    "\n",
    "priority_backlog = pd.DataFrame(priority_segments)\n",
    "\n",
    "print(\"\\nPRIORITY BACKLOG FOR MODULE 2.7:\")\n",
    "print(priority_backlog.to_string(index=False))\n",
    "\n",
    "# Save for next module\n",
    "priority_backlog.to_csv('priority_segments_for_2.7.csv', index=False)\n",
    "print(\"\\n✓ Priority backlog saved to 'priority_segments_for_2.7.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
