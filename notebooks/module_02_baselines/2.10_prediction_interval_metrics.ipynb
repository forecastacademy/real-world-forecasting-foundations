{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.10: Prediction Interval Metrics - Coverage, Width, and Winkler\n",
    "\n",
    "## OPENING: PREDICTION INTERVALS ARE PROMISES\n",
    "\n",
    "In the last video, we built prediction intervals. We generated model-based intervals from AutoETS and AutoTheta. We wrapped those same forecasts with conformal intervals calibrated to historical reality.\n",
    "\n",
    "But here's the question: **Can we trust them?**\n",
    "\n",
    "A prediction interval is a promise. It says: \"80% of the time, demand will fall inside this range.\"\n",
    "\n",
    "That's a big claim. And if the claim is wrong — if we set safety stock based on a lie — we stock out. Or we overstock. Either way, we lose money.\n",
    "\n",
    "So before we deploy any interval method in production, we audit it. We measure whether it kept its promise.\n",
    "\n",
    "**Forecasting isn't about being right. It's about being responsibly wrong.**\n",
    "\n",
    "Intervals are how we quantify that responsibility. Metrics are how we verify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP: Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_theme()\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction intervals and actuals from previous module\n",
    "intervals_df = pd.read_csv('prediction_intervals_80pct.csv')\n",
    "cv_forecasts = pd.read_csv('path/to/cv_forecasts.csv')\n",
    "\n",
    "print(f\"Loaded {len(intervals_df)} interval predictions\")\n",
    "print(f\"Columns: {intervals_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 1: THE TWO JOBS OF AN INTERVAL\n",
    "\n",
    "### Job 1: Validity (Calibration)\n",
    "\n",
    "**The question:** Does the interval hit the coverage level it claims?\n",
    "\n",
    "If we ask for 80%, do we actually get around 80%?\n",
    "\n",
    "This is the **promise**. An interval that says \"80%\" but only covers 65% of outcomes is lying.\n",
    "\n",
    "### Job 2: Efficiency (Usefulness)\n",
    "\n",
    "**The question:** Is the interval narrow enough to be operationally useful?\n",
    "\n",
    "An interval that says \"demand will be between 0 and 10,000 units\" might have perfect coverage — but it's useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 2: VALIDITY METRICS (DOES IT KEEP ITS PROMISE?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coverage(actuals, lower_bounds, upper_bounds, target_coverage=0.80):\n",
    "    \"\"\"\n",
    "    Calculate empirical coverage of prediction intervals\n",
    "    \n",
    "    Coverage = % of actuals that fall within [lower, upper]\n",
    "    \"\"\"\n",
    "    within_interval = (actuals >= lower_bounds) & (actuals <= upper_bounds)\n",
    "    coverage = within_interval.mean()\n",
    "    \n",
    "    # How close are we to target?\n",
    "    coverage_error = abs(coverage - target_coverage)\n",
    "    \n",
    "    return {\n",
    "        'coverage': coverage,\n",
    "        'target': target_coverage,\n",
    "        'coverage_error': coverage_error,\n",
    "        'status': 'PASS' if coverage_error < 0.05 else 'FAIL',\n",
    "        'num_within': within_interval.sum(),\n",
    "        'total': len(actuals)\n",
    "    }\n",
    "\n",
    "# Calculate coverage\n",
    "if all(col in intervals_df.columns for col in ['actual', 'lower_80', 'upper_80']):\n",
    "    coverage_metrics = calculate_coverage(\n",
    "        intervals_df['actual'].values,\n",
    "        intervals_df['lower_80'].values,\n",
    "        intervals_df['upper_80'].values,\n",
    "        target_coverage=0.80\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCOVERAGE VALIDITY TEST (Job 1: Does it keep its promise?):\")\n",
    "    print(f\"  Target Coverage: {coverage_metrics['target']:.1%}\")\n",
    "    print(f\"  Actual Coverage: {coverage_metrics['coverage']:.1%}\")\n",
    "    print(f\"  Coverage Error: {coverage_metrics['coverage_error']:.1%}\")\n",
    "    print(f\"  Result: {coverage_metrics['status']}\")\n",
    "    print(f\"  ({coverage_metrics['num_within']} out of {coverage_metrics['total']} actuals within interval)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpness(lower_bounds, upper_bounds):\n",
    "    \"\"\"\n",
    "    Calculate sharpness (tightness) of intervals\n",
    "    Lower width = sharper (better) intervals\n",
    "    \"\"\"\n",
    "    widths = upper_bounds - lower_bounds\n",
    "    \n",
    "    return {\n",
    "        'mean_width': widths.mean(),\n",
    "        'median_width': np.median(widths),\n",
    "        'min_width': widths.min(),\n",
    "        'max_width': widths.max(),\n",
    "        'std_width': widths.std()\n",
    "    }\n",
    "\n",
    "# Calculate sharpness\n",
    "if all(col in intervals_df.columns for col in ['lower_80', 'upper_80']):\n",
    "    sharpness_metrics = calculate_sharpness(\n",
    "        intervals_df['lower_80'].values,\n",
    "        intervals_df['upper_80'].values\n",
    "    )\n",
    "    \n",
    "    print(\"\\nSHARPNESS METRICS (Job 2: How useful is it?):\")\n",
    "    print(f\"  Mean Interval Width: ±{sharpness_metrics['mean_width']:.2f} units\")\n",
    "    print(f\"  Median Width: ±{sharpness_metrics['median_width']:.2f} units\")\n",
    "    print(f\"  Range: [{sharpness_metrics['min_width']:.2f}, {sharpness_metrics['max_width']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 3: INTERVAL SCORE (WINKLER SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_winkler_score(actuals, lower_bounds, upper_bounds, alpha=0.20):\n",
    "    \"\"\"\n",
    "    Winkler Score: Combines validity + efficiency\n",
    "    \n",
    "    WS = (Upper - Lower) + (2/α) * (Lower - Actual) if Actual < Lower\n",
    "                          + (2/α) * (Actual - Upper) if Actual > Upper\n",
    "    \n",
    "    Lower score = Better intervals (narrow + covers actual)\n",
    "    \n",
    "    Args:\n",
    "        alpha: 1 - confidence_level (0.20 for 80% confidence)\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = np.zeros(len(actuals))\n",
    "    \n",
    "    for i in range(len(actuals)):\n",
    "        width = upper_bounds[i] - lower_bounds[i]\n",
    "        \n",
    "        if actuals[i] < lower_bounds[i]:\n",
    "            # Actual below interval\n",
    "            penalty = (2 / alpha) * (lower_bounds[i] - actuals[i])\n",
    "            scores[i] = width + penalty\n",
    "        elif actuals[i] > upper_bounds[i]:\n",
    "            # Actual above interval\n",
    "            penalty = (2 / alpha) * (actuals[i] - upper_bounds[i])\n",
    "            scores[i] = width + penalty\n",
    "        else:\n",
    "            # Actual within interval\n",
    "            scores[i] = width\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Calculate Winkler scores\n",
    "if all(col in intervals_df.columns for col in ['actual', 'lower_80', 'upper_80']):\n",
    "    winkler_scores = calculate_winkler_score(\n",
    "        intervals_df['actual'].values,\n",
    "        intervals_df['lower_80'].values,\n",
    "        intervals_df['upper_80'].values,\n",
    "        alpha=0.20  # For 80% intervals\n",
    "    )\n",
    "    \n",
    "    print(\"\\nWINKLER SCORE (Combined Metric):\")\n",
    "    print(f\"  Mean Winkler Score: {winkler_scores.mean():.2f}\")\n",
    "    print(f\"  Median: {np.median(winkler_scores):.2f}\")\n",
    "    print(f\"  Std Dev: {winkler_scores.std():.2f}\")\n",
    "    print(f\"\\n  Interpretation:\")\n",
    "    print(f\"    - Penalizes intervals that miss actuals (validity)\")\n",
    "    print(f\"    - Penalizes wide intervals (efficiency)\")\n",
    "    print(f\"    - Lower score = Better intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 4: INTERVAL METRICS BY SEGMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_quality_scorecard(intervals_df, segment_column='category'):\n",
    "    \"\"\"\n",
    "    Generate interval quality metrics by segment\n",
    "    \"\"\"\n",
    "    \n",
    "    if segment_column not in intervals_df.columns:\n",
    "        print(f\"Column {segment_column} not found. Using overall metrics only.\")\n",
    "        return None\n",
    "    \n",
    "    scorecard = []\n",
    "    \n",
    "    for segment in intervals_df[segment_column].unique():\n",
    "        seg_data = intervals_df[intervals_df[segment_column] == segment]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        coverage = ((\n",
    "            (seg_data['actual'] >= seg_data['lower_80']) & \n",
    "            (seg_data['actual'] <= seg_data['upper_80'])\n",
    "        ).sum() / len(seg_data))\n",
    "        \n",
    "        width = (seg_data['upper_80'] - seg_data['lower_80']).mean()\n",
    "        \n",
    "        scorecard.append({\n",
    "            'segment': segment,\n",
    "            'coverage': coverage,\n",
    "            'interval_width': width,\n",
    "            'sample_size': len(seg_data)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(scorecard)\n",
    "\n",
    "# If intervals_df has category column, generate scorecard\n",
    "if 'category' in intervals_df.columns:\n",
    "    scorecard = interval_quality_scorecard(intervals_df, 'category')\n",
    "    print(\"\\nINTERVAL QUALITY BY SEGMENT:\")\n",
    "    print(scorecard.to_string(index=False))\nelse:\n",
    "    print(\"\\nCategory column not found. Add segment information to intervals_df for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 5: VISUALIZING INTERVAL PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interval_performance(intervals_df, sample_size=100):\n",
    "    \"\"\"\n",
    "    Visualize interval performance: coverage + width\n",
    "    \"\"\"\n",
    "    sample = intervals_df.head(sample_size)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Intervals vs Actuals\n",
    "    ax = axes[0]\n",
    "    x = range(len(sample))\n",
    "    \n",
    "    ax.plot(x, sample['actual'], 'ko-', label='Actual', markersize=4, linewidth=1)\n",
    "    ax.plot(x, sample['forecast'], 'b^-', label='Point Forecast', alpha=0.7, markersize=4)\n",
    "    ax.fill_between(x, sample['lower_80'], sample['upper_80'], \n",
    "                     alpha=0.3, color='blue', label='80% Interval')\n",
    "    \n",
    "    ax.set_title('Interval Coverage: Do Actuals Fall Within Predicted Range?')\n",
    "    ax.set_ylabel('Demand')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Interval width over time\n",
    "    widths = sample['upper_80'] - sample['lower_80']\n",
    "    ax = axes[1]\n",
    "    ax.bar(x, widths, color='orange', alpha=0.7)\n",
    "    ax.axhline(y=widths.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean Width: {widths.mean():.2f}')\n",
    "    ax.set_title('Interval Width (Sharpness): Narrower = More Useful')\n",
    "    ax.set_xlabel('Time Period')\n",
    "    ax.set_ylabel('Interval Width')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "plot_interval_performance(intervals_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 6: DECISION: DEPLOY OR RECALIBRATE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_audit_decision(coverage_metrics, sharpness_metrics):\n",
    "    \"\"\"\n",
    "    Make go/no-go decision based on interval metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    decision = {\n",
    "        'validity_pass': coverage_metrics['status'] == 'PASS',\n",
    "        'validity_score': f\"{coverage_metrics['coverage']:.1%} vs {coverage_metrics['target']:.1%}\",\n",
    "        'sharpness_score': f\"{sharpness_metrics['mean_width']:.2f} units\",\n",
    "        'recommendation': None\n",
    "    }\n",
    "    \n",
    "    # Decision logic\n",
    "    if decision['validity_pass']:\n",
    "        if sharpness_metrics['mean_width'] < 50:  # Threshold example\n",
    "            decision['recommendation'] = 'DEPLOY - Intervals are valid and sharp'\n",
    "        else:\n",
    "            decision['recommendation'] = 'DEPLOY with warning - Valid but wide intervals may reduce usefulness'\n",
    "    else:\n",
    "        if coverage_metrics['coverage'] < coverage_metrics['target']:\n",
    "            decision['recommendation'] = 'RECALIBRATE - Intervals are too narrow (missing actuals)'\n",
    "        else:\n",
    "            decision['recommendation'] = 'RECALIBRATE - Intervals are too wide (over-conservative)'\n",
    "    \n",
    "    return decision\n",
    "\n",
    "# Make decision\n",
    "if 'coverage_metrics' in locals() and 'sharpness_metrics' in locals():\n",
    "    decision = interval_audit_decision(coverage_metrics, sharpness_metrics)\n",
    "    \n",
    "    print(\"\\nINTERVAL AUDIT DECISION:\")\n",
    "    print(f\"\\n  Validity (Coverage): {decision['validity_score']}\")\n",
    "    print(f\"  Validity Status: {'✓ PASS' if decision['validity_pass'] else '✗ FAIL'}\")\n",
    "    print(f\"\\n  Efficiency (Sharpness): {decision['sharpness_score']}\")\n",
    "    print(f\"\\n  Decision: {decision['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 7: FINAL DELIVERABLE - INTERVAL AUDIT REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate audit report\n",
    "audit_report = f\"\"\"\n",
    "PREDICTION INTERVAL AUDIT REPORT\n",
    "{'='*70}\n",
    "\n",
    "OBJECTIVE\n",
    "--------\n",
    "Verify that 80% prediction intervals keep their promise:\n",
    "\"80% of the time, actuals fall within the predicted range\"\n",
    "\n",
    "RESULTS - VALIDITY (Job 1: Does it keep its promise?)\n",
    "---\n",
    "Target Coverage:     80%\n",
    "Actual Coverage:     {coverage_metrics['coverage']:.1%}\n",
    "Coverage Error:      {coverage_metrics['coverage_error']:.2%}\n",
    "Status:              {coverage_metrics['status']}\n",
    "\n",
    "RESULTS - EFFICIENCY (Job 2: Is it useful?)\n",
    "---\n",
    "Mean Interval Width: ±{sharpness_metrics['mean_width']:.2f} units\n",
    "Median Width:        ±{sharpness_metrics['median_width']:.2f} units\n",
    "Min Width:           ±{sharpness_metrics['min_width']:.2f} units\n",
    "Max Width:           ±{sharpness_metrics['max_width']:.2f} units\n",
    "\n",
    "COMBINED METRIC - WINKLER SCORE\n",
    "---\n",
    "Mean Score:          {winkler_scores.mean():.2f}\n",
    "Median Score:        {np.median(winkler_scores):.2f}\n",
    "(Lower is better)\n",
    "\n",
    "RECOMMENDATION\n",
    "---\n",
    "{decision['recommendation']}\n",
    "\n",
    "NEXT STEPS\n",
    "---\n",
    "1. Review this audit with stakeholders\n",
    "2. If deploying: Use intervals for safety stock decisions\n",
    "3. If recalibrating: Adjust interval method and re-test\n",
    "4. Monitor coverage in production (ongoing validation)\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(audit_report)\n",
    "\n",
    "# Save report\n",
    "with open('interval_audit_report_2.10.txt', 'w') as f:\n",
    "    f.write(audit_report)\n",
    "\n",
    "print(\"\\n✓ Audit report saved to 'interval_audit_report_2.10.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
