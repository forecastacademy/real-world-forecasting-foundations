{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.10: Diagnostics — Computing the Coordinates\n",
    "\n",
    "> **Goal:** Generate the coordinates for the portfolio map (but do not map yet).\n",
    "\n",
    "Before we can triage our portfolio, we need to measure each series along two dimensions:\n",
    "\n",
    "- **Structure:** How much predictable signal exists (trend, seasonality, information)\n",
    "- **Chaos:** How much unpredictable noise exists (entropy, lumpiness, gaps)\n",
    "\n",
    "This module computes those scores. The next module (1.12) will use them to build the strategic map.\n",
    "\n",
    "| This Module (1.10) | Next Module (1.12) |\n",
    "|-------------------|--------------------|\n",
    "| Compute metrics | Plot the map |\n",
    "| Normalize scores | Assign archetypes |\n",
    "| Sanity check distributions | Strategic triage |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete | Root: real-world-forecasting-foundations | Module: 1_10\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports ---\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Path Configuration (before local imports) ---\n",
    "MODULE_DIR = Path().resolve()\n",
    "PROJECT_ROOT = MODULE_DIR.parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# --- Local Imports ---\n",
    "import tsforge as tsf\n",
    "from src import (\n",
    "    CacheManager,\n",
    "    ArtifactManager,\n",
    "    get_notebook_name\n",
    ")\n",
    "\n",
    "# --- Settings ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "# --- Paths ---\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --- Managers ---\n",
    "NB_NAME = get_notebook_name()\n",
    "cache = CacheManager(PROJECT_ROOT / \".cache\" / NB_NAME)\n",
    "artifacts = ArtifactManager(PROJECT_ROOT / \"artifacts\")\n",
    "\n",
    "print(f\"✓ Setup complete | Root: {PROJECT_ROOT.name} | Module: {NB_NAME[:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded '1.08' from 01_foundations/\n",
      "   Shape: 6,848,887 × 20\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data from Module 1.08 ---\n",
    "df = artifacts.load('1.08')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Context: What is XYZ Segmentation?\n",
    "\n",
    "In classic inventory management, **ABC-XYZ segmentation** combines two dimensions:\n",
    "\n",
    "- **ABC (Volume):** How much does this item sell? (A = high, C = low)\n",
    "- **XYZ (Predictability):** How forecastable is demand? (X = stable, Z = erratic)\n",
    "\n",
    "We've already computed volume metrics. Now we need the **XYZ dimension** — but instead of simple CV (coefficient of variation), we'll use a richer characterization:\n",
    "\n",
    "### Our Approach: Structure vs Chaos\n",
    "\n",
    "We measure predictability along two axes using our **LD6 metrics** (Lean Diagnostics, 6 metrics):\n",
    "\n",
    "| Dimension | What It Measures | High Score Means |\n",
    "|-----------|------------------|------------------|\n",
    "| **Structure** | Predictable patterns | Strong trend, clear seasonality, high signal |\n",
    "| **Chaos** | Unpredictable noise | High entropy, lumpy demand, sparse data |\n",
    "\n",
    "This gives us a 2D map instead of a 1D ranking — much more actionable for forecasting strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Define Metric Groups\n",
    "\n",
    "We separate our LD6 metrics into two groups based on what they measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure Group: ['trend_strength', 'seasonal_strength', 'mutual_information']\n",
      "Chaos Group: ['permutation_entropy', 'lumpiness', 'intermittency_adi']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# METRIC DEFINITIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Structure metrics: indicators of predictable, learnable patterns\n",
    "STRUCTURE_METRICS = [\n",
    "    'trend_strength',      # How strong is the directional movement?\n",
    "    'seasonal_strength',   # How strong is the repeating pattern?\n",
    "    'mutual_information'   # How much signal is in the lagged values?\n",
    "]\n",
    "\n",
    "# Chaos metrics: indicators of noise, randomness, sparsity\n",
    "CHAOS_METRICS = [\n",
    "    'permutation_entropy', # How random is the ordering of values?\n",
    "    'lumpiness',           # How inconsistent are the variance regimes?\n",
    "    'intermittency_adi'    # How sparse is the demand? (avg days between sales)\n",
    "]\n",
    "\n",
    "# Combined list for computation\n",
    "LD6_METRICS = STRUCTURE_METRICS + CHAOS_METRICS\n",
    "\n",
    "print(\"Structure Group:\", STRUCTURE_METRICS)\n",
    "print(\"Chaos Group:\", CHAOS_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Interpretation Guide\n",
    "\n",
    "| Metric | Group | What It Measures | High Value Means |\n",
    "|--------|-------|------------------|------------------|\n",
    "| `trend_strength` | Structure | Strength of directional movement | Clear up/down trend |\n",
    "| `seasonal_strength` | Structure | Strength of repeating cycles | Predictable seasonal pattern |\n",
    "| `mutual_information` | Structure | Information in lagged values | Past predicts future |\n",
    "| `permutation_entropy` | Chaos | Randomness of value ordering | Highly disordered, noisy |\n",
    "| `lumpiness` | Chaos | Variance instability over time | Unstable, regime-switching |\n",
    "| `intermittency_adi` | Chaos | Average inter-demand interval | Sparse, lots of zeros |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Compute LD6 Metrics\n",
    "\n",
    "We compute all six metrics for every series in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# Check cache first\n",
    "metrics_raw = cache.load('ld6_metrics_raw')\n",
    "\n",
    "if metrics_raw is None:\n",
    "    print(\"Computing LD6 metrics (this may take a few minutes)...\")\n",
    "    \n",
    "    # Compute metrics using tsforge\n",
    "    metrics_raw = tsf.compute_features(\n",
    "        df=df,\n",
    "        id_col='unique_id',\n",
    "        date_col='ds',\n",
    "        value_col='y',\n",
    "        features=LD6_METRICS,\n",
    "        seasonal_period=52  # Weekly data with annual seasonality\n",
    "    )\n",
    "    \n",
    "    cache.save(metrics_raw, 'ld6_metrics_raw')\n",
    "    print(f\"✓ Computed and cached metrics for {len(metrics_raw):,} series\")\n",
    "else:\n",
    "    print(f\"✓ Loaded cached metrics for {len(metrics_raw):,} series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview raw metrics\n",
    "print(\"Raw Metrics Preview:\")\n",
    "print(\"=\" * 60)\n",
    "metrics_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values\n",
    "print(\"Missing Values Check:\")\n",
    "print(metrics_raw[LD6_METRICS].isnull().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nRaw Metric Statistics:\")\n",
    "metrics_raw[LD6_METRICS].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Normalize Metrics\n",
    "\n",
    "Before averaging metrics into scores, we need to normalize them:\n",
    "\n",
    "1. **Clip outliers:** Cap chaos metrics at 95th percentile to prevent extreme values from dominating\n",
    "2. **Min-Max scale:** Transform all metrics to 0-1 range for fair comparison\n",
    "\n",
    "This ensures that no single metric dominates the score just because of its scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: CLIP CHAOS METRICS AT 95TH PERCENTILE\n",
    "# =============================================================================\n",
    "\n",
    "metrics_clipped = metrics_raw.copy()\n",
    "\n",
    "print(\"Clipping chaos metrics at 95th percentile:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col in CHAOS_METRICS:\n",
    "    p95 = metrics_raw[col].quantile(0.95)\n",
    "    original_max = metrics_raw[col].max()\n",
    "    \n",
    "    # Clip values above 95th percentile\n",
    "    metrics_clipped[col] = metrics_raw[col].clip(upper=p95)\n",
    "    \n",
    "    n_clipped = (metrics_raw[col] > p95).sum()\n",
    "    print(f\"  {col}: max {original_max:.3f} → {p95:.3f} ({n_clipped:,} values clipped)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: MIN-MAX SCALING (0 TO 1)\n",
    "# =============================================================================\n",
    "\n",
    "metrics_normalized = metrics_clipped.copy()\n",
    "\n",
    "print(\"\\nApplying min-max normalization:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col in LD6_METRICS:\n",
    "    col_min = metrics_clipped[col].min()\n",
    "    col_max = metrics_clipped[col].max()\n",
    "    col_range = col_max - col_min\n",
    "    \n",
    "    if col_range > 0:\n",
    "        metrics_normalized[col] = (metrics_clipped[col] - col_min) / col_range\n",
    "    else:\n",
    "        # If all values are the same, set to 0.5\n",
    "        metrics_normalized[col] = 0.5\n",
    "    \n",
    "    print(f\"  {col}: [{col_min:.3f}, {col_max:.3f}] → [0, 1]\")\n",
    "\n",
    "print(\"\\n✓ All metrics now on 0-1 scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify normalization\n",
    "print(\"Normalized Metric Ranges:\")\n",
    "for col in LD6_METRICS:\n",
    "    print(f\"  {col}: [{metrics_normalized[col].min():.3f}, {metrics_normalized[col].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Calculate Super-Scores\n",
    "\n",
    "Now we create two final coordinates by averaging the normalized metrics in each group:\n",
    "\n",
    "- **Structure Score** = mean(trend_strength, seasonal_strength, mutual_information)\n",
    "- **Chaos Score** = mean(permutation_entropy, lumpiness, intermittency_adi)\n",
    "\n",
    "**Interpretation:**\n",
    "- High Structure Score → Strong predictable patterns\n",
    "- High Chaos Score → High noise/randomness/sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CALCULATE SUPER-SCORES\n",
    "# =============================================================================\n",
    "\n",
    "# Structure Score: average of structure metrics\n",
    "metrics_normalized['structure_score'] = metrics_normalized[STRUCTURE_METRICS].mean(axis=1)\n",
    "\n",
    "# Chaos Score: average of chaos metrics  \n",
    "metrics_normalized['chaos_score'] = metrics_normalized[CHAOS_METRICS].mean(axis=1)\n",
    "\n",
    "print(\"Super-Score Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nStructure Score:\")\n",
    "print(f\"  Mean: {metrics_normalized['structure_score'].mean():.3f}\")\n",
    "print(f\"  Std:  {metrics_normalized['structure_score'].std():.3f}\")\n",
    "print(f\"  Range: [{metrics_normalized['structure_score'].min():.3f}, {metrics_normalized['structure_score'].max():.3f}]\")\n",
    "\n",
    "print(f\"\\nChaos Score:\")\n",
    "print(f\"  Mean: {metrics_normalized['chaos_score'].mean():.3f}\")\n",
    "print(f\"  Std:  {metrics_normalized['chaos_score'].std():.3f}\")\n",
    "print(f\"  Range: [{metrics_normalized['chaos_score'].min():.3f}, {metrics_normalized['chaos_score'].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the scored dataframe\n",
    "score_cols = ['unique_id'] + LD6_METRICS + ['structure_score', 'chaos_score']\n",
    "scores_df = metrics_normalized[score_cols].copy()\n",
    "\n",
    "print(\"Scored DataFrame Preview:\")\n",
    "scores_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Sanity Check: Metric Distributions\n",
    "\n",
    "Before passing these scores to the next module, let's verify the distributions look reasonable.\n",
    "\n",
    "**What we're looking for:**\n",
    "- No extreme skew (normalization worked)\n",
    "- Reasonable spread (metrics differentiate series)\n",
    "- No unexpected spikes at 0 or 1 (unless expected, like intermittency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INDIVIDUAL METRIC DISTRIBUTIONS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "fig.suptitle('Normalized LD6 Metric Distributions', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Structure metrics (top row)\n",
    "for idx, col in enumerate(STRUCTURE_METRICS):\n",
    "    ax = axes[0, idx]\n",
    "    ax.hist(metrics_normalized[col], bins=50, color='#2E86AB', edgecolor='white', alpha=0.8)\n",
    "    ax.set_title(col, fontsize=11)\n",
    "    ax.set_xlabel('Normalized Value')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.axvline(metrics_normalized[col].mean(), color='red', linestyle='--', label=f'mean={metrics_normalized[col].mean():.2f}')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Chaos metrics (bottom row)  \n",
    "for idx, col in enumerate(CHAOS_METRICS):\n",
    "    ax = axes[1, idx]\n",
    "    ax.hist(metrics_normalized[col], bins=50, color='#E94F37', edgecolor='white', alpha=0.8)\n",
    "    ax.set_title(col, fontsize=11)\n",
    "    ax.set_xlabel('Normalized Value')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.axvline(metrics_normalized[col].mean(), color='darkred', linestyle='--', label=f'mean={metrics_normalized[col].mean():.2f}')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Add row labels\n",
    "axes[0, 0].set_ylabel('STRUCTURE\\nCount', fontsize=10)\n",
    "axes[1, 0].set_ylabel('CHAOS\\nCount', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUPER-SCORE DISTRIBUTIONS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.suptitle('Super-Score Distributions', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Structure Score\n",
    "ax = axes[0]\n",
    "ax.hist(metrics_normalized['structure_score'], bins=50, color='#2E86AB', edgecolor='white', alpha=0.8)\n",
    "ax.axvline(0.5, color='gray', linestyle=':', linewidth=2, label='midpoint')\n",
    "ax.axvline(metrics_normalized['structure_score'].mean(), color='red', linestyle='--', \n",
    "           label=f'mean={metrics_normalized[\"structure_score\"].mean():.2f}')\n",
    "ax.set_title('Structure Score', fontsize=12)\n",
    "ax.set_xlabel('Score (0=Low Structure, 1=High Structure)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend()\n",
    "\n",
    "# Chaos Score\n",
    "ax = axes[1]\n",
    "ax.hist(metrics_normalized['chaos_score'], bins=50, color='#E94F37', edgecolor='white', alpha=0.8)\n",
    "ax.axvline(0.5, color='gray', linestyle=':', linewidth=2, label='midpoint')\n",
    "ax.axvline(metrics_normalized['chaos_score'].mean(), color='darkred', linestyle='--',\n",
    "           label=f'mean={metrics_normalized[\"chaos_score\"].mean():.2f}')\n",
    "ax.set_title('Chaos Score', fontsize=12)\n",
    "ax.set_xlabel('Score (0=Low Chaos, 1=High Chaos)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRELATION CHECK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Metric Correlations:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nCorrelation between Structure and Chaos scores:\")\n",
    "corr = metrics_normalized['structure_score'].corr(metrics_normalized['chaos_score'])\n",
    "print(f\"  r = {corr:.3f}\")\n",
    "\n",
    "if abs(corr) < 0.3:\n",
    "    print(\"  → Low correlation: good! These measure different things.\")\n",
    "elif abs(corr) < 0.6:\n",
    "    print(\"  → Moderate correlation: some overlap, but still useful separation.\")\n",
    "else:\n",
    "    print(\"  → High correlation: warning - these may be measuring similar things.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Output & Save\n",
    "\n",
    "We now have our scored dataframe ready for the portfolio architecture analysis in Module 1.12.\n",
    "\n",
    "**Output columns:**\n",
    "- `unique_id`: Series identifier\n",
    "- 6 normalized LD6 metrics\n",
    "- `structure_score`: Aggregate structure measure (0-1)\n",
    "- `chaos_score`: Aggregate chaos measure (0-1)\n",
    "\n",
    "**NOT included (saved for 1.12):**\n",
    "- ❌ No Structure × Chaos scatter plot\n",
    "- ❌ No quadrant assignments\n",
    "- ❌ No archetype labels (Stable/Complex/Messy/Low Signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL OUTPUT\n",
    "# =============================================================================\n",
    "\n",
    "# Prepare final output dataframe\n",
    "output_cols = ['unique_id'] + LD6_METRICS + ['structure_score', 'chaos_score']\n",
    "scores_output = metrics_normalized[output_cols].copy()\n",
    "\n",
    "print(\"Final Output Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {scores_output.shape[0]:,} series × {scores_output.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {list(scores_output.columns)}\")\n",
    "print(f\"\\nPreview:\")\n",
    "scores_output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as artifact for Module 1.12\n",
    "artifacts.save(\n",
    "    scores_output, \n",
    "    name='1.10',\n",
    "    description='LD6 diagnostic scores (structure + chaos) for portfolio segmentation'\n",
    ")\n",
    "print(\"\\n✓ Saved artifact '1.10' for use in Module 1.12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**What we computed:**\n",
    "\n",
    "1. **LD6 Metrics** — Six diagnostic metrics capturing structure and chaos\n",
    "2. **Normalization** — Clipped outliers + min-max scaling for fair comparison\n",
    "3. **Super-Scores** — Two aggregate coordinates for each series\n",
    "\n",
    "**What the distributions tell us:**\n",
    "\n",
    "- Structure scores show [describe observed pattern]\n",
    "- Chaos scores show [describe observed pattern]\n",
    "- The two dimensions are [correlated/uncorrelated] — [interpretation]\n",
    "\n",
    "**What's next:**\n",
    "\n",
    "Module 1.12 will use these scores to:\n",
    "- Plot the Structure × Chaos map\n",
    "- Assign each series to an archetype\n",
    "- Identify representative \"hero\" series\n",
    "- Build the portfolio risk matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next: Module 1.12\n",
    "\n",
    "**Portfolio Architecture — The Strategic View**\n",
    "\n",
    "- Plot the 2×2 Structure × Chaos map\n",
    "- Assign archetypes: Stable / Complex / Messy / Low Signal\n",
    "- Select and validate hero series\n",
    "- Build the Department × Archetype risk matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_forge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
