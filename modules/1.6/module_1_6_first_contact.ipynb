{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.6: First Contact with the Data\n",
    "\n",
    "> **Goal:** Run first-contact checks to confirm the data supports the 5Q Framework, then clean and aggregate to weekly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tsforge as tsf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "`messify=True` simulates real-world data issues (string dtypes, NaN injection, duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING M5 DATA\n",
      "======================================================================\n",
      "‚úì M5 cache detected. Loading from local files...\n",
      "‚úì Loaded in 1.1s\n",
      "  Shape: 47,649,940 rows √ó 3 columns\n",
      "  Memory: 638.4 MB\n",
      "  Columns: unique_id, ds, y\n",
      "  Returning: Y_df, X_df, S_df (all 3 dataframes)\n",
      "\n",
      "üîß Applying messification...\n",
      "======================================================================\n",
      "LOADING CACHED MESSIFIED DATA\n",
      "======================================================================\n",
      "\n",
      "üìÅ Cache file: m5_messy_n30490_rs42_zna30_dup150_dtype1_rmv2_dropna10.parquet\n",
      "   Using cached version (skip messification)\n",
      "\n",
      "üí° To regenerate: set force_refresh=True\n",
      "\n",
      "‚úì Loaded 46,250,639 rows √ó 3 columns\n",
      "======================================================================\n",
      "  Expanding hierarchy via S_df merge...\n",
      "  ‚úì Added hierarchy columns: ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
      "\n",
      "======================================================================\n",
      "LOAD COMPLETE\n",
      "======================================================================\n",
      "  Shape: 46,250,639 rows √ó 7 columns\n",
      "  Columns: ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'ds', 'y']\n",
      "  Applied: messified, hierarchy columns\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "df = tsf.load_m5(\n",
    "    DATA_DIR,\n",
    "    messify=True,\n",
    "    messify_kwargs={\n",
    "        'random_state': 42,\n",
    "        'zero_to_na_pct': 0.30,\n",
    "        'add_duplicates': True,\n",
    "        'n_duplicates': 150,\n",
    "        'corrupt_dtypes': True,\n",
    "        'drop_na_frac': 0.10,  # Drop 10% of NA rows to simulate incomplete data\n",
    "        'cache_dir': DATA_DIR\n",
    "    },\n",
    "    include_hierarchy=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Aggregation Checks (Daily Data)\n",
    "\n",
    "Fix these issues BEFORE aggregating ‚Äî otherwise they corrupt the weekly rollup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Fix Data Types & Handle NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messification converts `ds` and `y` to strings with literal `\"nan\"` values. Convert back to proper types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds    object\n",
       "y     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['ds', 'y']].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`errors='coerce'` converts unparseable values (including string `\"nan\"`) to proper NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check remaining NAs. We'll handle imputation in Module 1.10 after filling gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id           0\n",
       "dept_id           0\n",
       "cat_id            0\n",
       "store_id          0\n",
       "state_id          0\n",
       "ds                0\n",
       "y           7495875\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check for Null IDs\n",
    "\n",
    "Rows with null ID columns can't be properly aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id     0\n",
       "dept_id     0\n",
       "cat_id      0\n",
       "store_id    0\n",
       "state_id    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values in ID columns\n",
    "col_ids = [c for c in df.columns if c not in ['ds', 'y']]\n",
    "df[col_ids].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Check for Weird Dates\n",
    "\n",
    "Look for dates before 1900, future dates, or outlier dates far from the main range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0   2011-01-29\n",
       " 1   2011-01-30\n",
       " 2   2011-01-31\n",
       " 3   2011-02-01\n",
       " 4   2011-02-02\n",
       " Name: ds, dtype: datetime64[ns],\n",
       " 1915   2016-06-15\n",
       " 1916   2016-06-16\n",
       " 1917   2016-06-17\n",
       " 1918   2016-06-18\n",
       " 1919   2016-06-19\n",
       " Name: ds, dtype: datetime64[ns])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_dates = (\n",
    "    df['ds']\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .sort_values()\n",
    ")\n",
    "\n",
    "unique_dates.head(5), unique_dates.tail(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data Info & Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46250639 entries, 0 to 46250638\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   item_id   category      \n",
      " 1   dept_id   category      \n",
      " 2   cat_id    category      \n",
      " 3   store_id  category      \n",
      " 4   state_id  category      \n",
      " 5   ds        datetime64[ns]\n",
      " 6   y         float64       \n",
      "dtypes: category(5), datetime64[ns](1), float64(1)\n",
      "memory usage: 970.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Check for Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A duplicate = same row (except target) appears multiple times. Remove before aggregating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'ds', 'y'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates on all columns except target\n",
    "non_target_cols = [c for c in df.columns if c != 'y']\n",
    "dup_mask = df.duplicated(subset=non_target_cols, keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    46250357\n",
       "True          282\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_mask.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=non_target_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Daily Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm daily data looks correct before aggregating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-05</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-06</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2011-02-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_id  dept_id cat_id store_id state_id         ds    y\n",
       "0  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-01-29  3.0\n",
       "1  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-01-30  0.0\n",
       "2  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-01-31  0.0\n",
       "3  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-01  1.0\n",
       "4  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-02  4.0\n",
       "5  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-03  2.0\n",
       "6  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-04  NaN\n",
       "7  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-05  2.0\n",
       "8  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-06  0.0\n",
       "9  FOODS_1_001  FOODS_1  FOODS     CA_1       CA 2011-02-07  0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregate to Weekly\n",
    "\n",
    "Weekly granularity aligns with business planning and reduces daily noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by all non-target, non-date columns and sum the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns except ds and y\n",
    "group_cols = [c for c in df.columns if c not in ['ds', 'y']]\n",
    "\n",
    "# Create week column\n",
    "# W-SAT = weeks ending Saturday = weeks starting Sunday (M5 convention)\n",
    "df['week'] = df['ds'].dt.to_period('W-SAT').dt.start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-07-14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-07-21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-07-28</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-08-04</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-08-11</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-08-18</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-08-25</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-09-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2013-09-15</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_id    dept_id   cat_id store_id state_id         ds    y\n",
       "0  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-07-14  1.0\n",
       "1  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-07-21  0.0\n",
       "2  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-07-28  2.0\n",
       "3  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-08-04  2.0\n",
       "4  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-08-11  6.0\n",
       "5  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-08-18  1.0\n",
       "6  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-08-25  2.0\n",
       "7  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-09-01  5.0\n",
       "8  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-09-08  1.0\n",
       "9  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1       CA 2013-09-15  5.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate: group by all id columns + week, sum the target\n",
    "df = (\n",
    "    df.groupby(group_cols + ['week'], as_index=False, observed=True)\n",
    "    ['y']\n",
    "    .sum()\n",
    "    .rename(columns={'week': 'ds'})\n",
    ")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Memory after aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6848061 entries, 0 to 6848060\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   item_id   category      \n",
      " 1   dept_id   category      \n",
      " 2   cat_id    category      \n",
      " 3   store_id  category      \n",
      " 4   state_id  category      \n",
      " 5   ds        datetime64[ns]\n",
      " 6   y         float64       \n",
      "dtypes: category(5), datetime64[ns](1), float64(1)\n",
      "memory usage: 143.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Date Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need 2-3x forecast horizon for meaningful patterns. For 12-week forecast, want ~36 weeks minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2011-01-23 00:00:00')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min date\n",
    "df['ds'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2016-06-19 00:00:00')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max date\n",
    "df['ds'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of weeks\n",
    "((df['ds'].max() - df['ds'].min()).days // 7) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. First Contact Summary\n",
    "\n",
    "Run all checks with a single function call `first_contact_check()` from `tsforge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FIRST CONTACT CHECK\n",
      "============================================================\n",
      "‚úì Required columns present (ds, y)\n",
      "‚úì ds is datetime\n",
      "‚úì y is numeric\n",
      "‚úì No NAs in ds\n",
      "‚úì No NAs in ID columns\n",
      "‚Ñπ 0 NAs in y (will impute in Module 1.10)\n",
      "‚úì No impossible dates\n",
      "‚úì No duplicates\n",
      "\n",
      "Summary:\n",
      "  Shape: 6,848,061 rows √ó 7 columns\n",
      "  Series: 30,490\n",
      "  Date range: 2011-01-23 to 2016-06-19\n",
      "  Unique dates: 283\n",
      "  Memory: 143.9 MB\n",
      "\n",
      "============================================================\n",
      "‚úì ALL CHECKS PASSED\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsf.first_contact_check_simple(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Output\n",
    "\n",
    "Save cleaned weekly data for Module 1.7. NAs preserved for gap-filling in Module 1.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved to data/1_6_output.parquet\n",
      "  Shape: 6,848,061 rows √ó 7 columns\n",
      "  Columns: ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'ds', 'y']\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned weekly data\n",
    "output_path = DATA_DIR / '1_6_output.parquet'\n",
    "df.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"‚úì Saved to {output_path}\")\n",
    "print(f\"  Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"  Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "| Module | Focus |\n",
    "|--------|-------|\n",
    "| **1.7** | Understand M5 structure (hierarchy, calendar, prices) |\n",
    "| **1.8** | Diagnostics (seasonality, volatility, trend) |\n",
    "| **1.9** | Portfolio analysis with GenAI |\n",
    "| **1.10** | Data preparation (fill gaps, calendar merge, imputation) |\n",
    "| **1.11** | Plotting & visual diagnostics |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_forge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
