{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.8: Timeseries Diagnostics\n",
    "\n",
    "> **Goal:** Explore characteristics of the M5 dataset using tsfeatures + tsforge.\n",
    "\n",
    "This module teaches you to:\n",
    "1. Load data\n",
    "2. Compute diagnostics at the most granular \"unique_id\" level. \n",
    "3. Motivate the focus on the \"Lie Detector Six\" metric set.\n",
    "    * getting a feel for the forecastability, quality and characteristics of the data BEFORE we start forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from tsforge import load_m5\n",
    "import tsforge as tsf\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook agenda \n",
    "\n",
    "* Reconfirm the unique_id definition\n",
    "* Compute tsfeatures + tsforge diagnostics per unique_id\n",
    "* Highlight the “Lie Detector Six”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data \n",
    "weekly_df = pd.read_parquet(\n",
    "    \"/Users/jackrodenberg/Desktop/real-world-forecasting-foundations/modules/output/m5_weekly_clean.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-02-05</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-02-12</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-02-26</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          unique_id         ds     y\n",
       "0  FOODS_1_001_CA_1 2011-01-29   3.0\n",
       "1  FOODS_1_001_CA_1 2011-02-05   9.0\n",
       "2  FOODS_1_001_CA_1 2011-02-12   7.0\n",
       "3  FOODS_1_001_CA_1 2011-02-19   8.0\n",
       "4  FOODS_1_001_CA_1 2011-02-26  14.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsforge.eda.ts_features_extension import permutation_entropy,MI_top_k_lags,ADI\n",
    "from tsfeatures import tsfeatures,lumpiness,stl_features,statistics\n",
    "# using nixtla's tsfeatures \n",
    "id_lvl_feats = tsfeatures(\n",
    "\n",
    "    ts = weekly_df,\n",
    "    # frequency of data is weekly, so here we input 52     \n",
    "    freq=52,\n",
    "\n",
    "    # COMPUTE LIE detector six \n",
    "    features=[\n",
    "        statistics,\n",
    "        lumpiness, # variance of variances \n",
    "        permutation_entropy, # permutation entropy \n",
    "        MI_top_k_lags, # sum of MI over top 5 lags \n",
    "        stl_features, # STL decomposition Features (Trend, Seasonal Strength)\n",
    "        ADI, # Avg Demand Interval\n",
    "        ],\n",
    "\n",
    "        scale=False # ENSURE YOU TURN THIS OFF for accurate statistics, otherwise outputs are standard scaled for model training.. \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* taking a closter look at the table we the \"Lie Detector 6\". \n",
    "\n",
    "    - Lumpiness: Variance of Variances \n",
    "    - Entropy (Permutation Entropy)\n",
    "    - Seasonal Strength \n",
    "    - Trend Strength\n",
    "    - MI Top K Lags: Mutual Information Top K Lags (K = 5)\n",
    "        - for more clarity this is the sum of the Mutual Information of the top 5 lags from lags 1-freq\n",
    "    - ADI: Average Demand Interval (time between demands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>lumpiness</th>\n",
       "      <th>permutation_entropy</th>\n",
       "      <th>seasonal_strength</th>\n",
       "      <th>trend</th>\n",
       "      <th>MI_top_k_lags</th>\n",
       "      <th>adi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>87.235596</td>\n",
       "      <td>0.969347</td>\n",
       "      <td>0.376623</td>\n",
       "      <td>0.204450</td>\n",
       "      <td>0.270401</td>\n",
       "      <td>1.105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_001_CA_2</td>\n",
       "      <td>230.382385</td>\n",
       "      <td>0.981118</td>\n",
       "      <td>0.439298</td>\n",
       "      <td>0.223280</td>\n",
       "      <td>0.153054</td>\n",
       "      <td>1.105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_001_CA_3</td>\n",
       "      <td>116.775986</td>\n",
       "      <td>0.984305</td>\n",
       "      <td>0.384099</td>\n",
       "      <td>0.162804</td>\n",
       "      <td>0.150131</td>\n",
       "      <td>1.118577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_001_CA_4</td>\n",
       "      <td>0.956493</td>\n",
       "      <td>0.952965</td>\n",
       "      <td>0.479389</td>\n",
       "      <td>0.110839</td>\n",
       "      <td>0.284070</td>\n",
       "      <td>1.276018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_001_TX_1</td>\n",
       "      <td>20.594612</td>\n",
       "      <td>0.962183</td>\n",
       "      <td>0.376637</td>\n",
       "      <td>0.260977</td>\n",
       "      <td>0.168827</td>\n",
       "      <td>1.200855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          unique_id   lumpiness  permutation_entropy  seasonal_strength  \\\n",
       "0  FOODS_1_001_CA_1   87.235596             0.969347           0.376623   \n",
       "1  FOODS_1_001_CA_2  230.382385             0.981118           0.439298   \n",
       "2  FOODS_1_001_CA_3  116.775986             0.984305           0.384099   \n",
       "3  FOODS_1_001_CA_4    0.956493             0.952965           0.479389   \n",
       "4  FOODS_1_001_TX_1   20.594612             0.962183           0.376637   \n",
       "\n",
       "      trend  MI_top_k_lags       adi  \n",
       "0  0.204450       0.270401  1.105469  \n",
       "1  0.223280       0.153054  1.105469  \n",
       "2  0.162804       0.150131  1.118577  \n",
       "3  0.110839       0.284070  1.276018  \n",
       "4  0.260977       0.168827  1.200855  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lvl_feats[[\"unique_id\",\"lumpiness\", \"permutation_entropy\", \"seasonal_strength\", \"trend\", \"MI_top_k_lags\", \"adi\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* add a few useful descriptors to help us understand the data in a more intuitive way. \n",
    "    - how much does each item make up of the total demand\n",
    "    - where does an item rank in terms of total sales? \n",
    "    - skewness and kurtosis (understand distribution shape)\n",
    "        - kurtosis: how heavy are the tails of the distribution ? \n",
    "        - skewness: how asymmetric is the distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some additional useful descriptors\n",
    "id_lvl_feats = id_lvl_feats.assign(\n",
    "    pct_of_demand=id_lvl_feats[\"total_sum\"] / id_lvl_feats[\"total_sum\"].sum(),\n",
    ")\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "# merge with skew, kurtosis of demand!\n",
    "id_lvl_feats = id_lvl_feats.merge(\n",
    "    weekly_df.groupby(\"unique_id\").agg(\n",
    "        skew=(\"y\", \"skew\"),\n",
    "        kurtosis=(\"y\", st.kurtosis),\n",
    "    ),\n",
    "    on=\"unique_id\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_six = [\n",
    "        \"lumpiness\",\n",
    "        \"permutation_entropy\",\n",
    "        \"seasonal_strength\",\n",
    "        \"trend\",\n",
    "        \"MI_top_k_lags\",\n",
    "        \"adi\",\n",
    "    ]\n",
    "\n",
    "# Absolute variants used for the second set of detectors\n",
    "id_lvl_feats[\"trend_two\"] = id_lvl_feats[\"linearity\"].abs()\n",
    "id_lvl_feats[\"seasonal_two\"] = id_lvl_feats[\"seasonal_pacf\"].abs()\n",
    "\n",
    "descriptors = ['unique_id','sales_rank','skew','kurtosis','pct_of_demand']\n",
    "\n",
    "cols = descriptors + ld_six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets examine the top 25% of EACH of the lie detector 6, and look to spot some trends!\n",
    "for col in ld_six:\n",
    "    print(f'Inspecting Top 25% of timeseries for {col}')\n",
    "    res = id_lvl_feats.loc[id_lvl_feats[col] >= id_lvl_feats[col].quantile(0.75),\n",
    "    ['unique_id','skew','kurtosis','pct_of_demand','lumpiness','adi','MI_top_k_lags']].set_index(\"unique_id\")\n",
    "\n",
    "    corr = res[[\"lumpiness\", \"adi\", \"MI_top_k_lags\", \"pct_of_demand\"]].corr(method=\"spearman\")\n",
    "\n",
    "    threshold = 0.7  # adjust as needed\n",
    "    high_corr = (\n",
    "        corr.where(\n",
    "            np.triu(np.ones(corr.shape), k=1).astype(bool)\n",
    "        )  # keep upper triangle without diagonal\n",
    "        .stack()\n",
    "        .rename(\"corr\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    high_corr = high_corr.loc[high_corr[\"corr\"].abs() >= threshold]\n",
    "\n",
    "    if not high_corr.empty:\n",
    "        print(f\"Pairs with |corr| >= {threshold}:\")\n",
    "        display(high_corr.sort_values(\"corr\", key=np.abs, ascending=False))\n",
    "        \n",
    "\n",
    "    display(res.describe().loc[['mean','std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lumpiness_scaled</td>\n",
       "      <td>pct_of_demand</td>\n",
       "      <td>0.844968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>permutation_entropy</td>\n",
       "      <td>adi</td>\n",
       "      <td>-0.923062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               level_0        level_1      corr\n",
       "5     lumpiness_scaled  pct_of_demand  0.844968\n",
       "9  permutation_entropy            adi -0.923062"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create individual detector flags\n",
    "trend_detectors = ld_six + [\"trend_two\", \"seasonal_two\"]\n",
    "\n",
    "for detector in trend_detectors:\n",
    "    id_lvl_feats[f\"{detector}_flag\"] = id_lvl_feats[detector] > id_lvl_feats[detector].quantile(\n",
    "        0.75\n",
    "    )\n",
    "\n",
    "# Build labeled dataset with prominent flags + detector details\n",
    "flag_cols = [f\"{d}_flag\" for d in trend_detectors]\n",
    "\n",
    "id_lvl_feats_labeled = id_lvl_feats.assign(\n",
    "    # Prominent characteristic flags\n",
    "    intermittent=id_lvl_feats[\"adi\"] >= 1.34,\n",
    "    heavy_tailed=id_lvl_feats[\"kurtosis\"].abs() > 3,\n",
    "    non_zero_min=id_lvl_feats[\"min\"] > 0,\n",
    "    # Detector summary flags\n",
    "    n_flags=id_lvl_feats[flag_cols].sum(axis=1),\n",
    "    suspect=id_lvl_feats[flag_cols].any(axis=1),\n",
    "    highly_suspect=lambda df: df[\"n_flags\"] >= 2,\n",
    "    # Which detectors are flagging\n",
    "    flagged_detectors=id_lvl_feats[flag_cols].apply(\n",
    "        lambda row: [trend_detectors[i] for i, val in enumerate(row) if val], axis=1\n",
    "    ),\n",
    "    # Compact string representation\n",
    "    flag_pattern=lambda df: df[\"flagged_detectors\"].apply(\n",
    "        lambda x: \"|\".join([d[:4].upper() for d in x]) if x else \"CLEAN\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Quick summary\n",
    "print(\"Characteristic flags:\")\n",
    "print(f\"  Intermittent: {id_lvl_feats_labeled['intermittent'].sum()}\")\n",
    "print(f\"  Heavy-tailed: {id_lvl_feats_labeled['heavy_tailed'].sum()}\")\n",
    "print(f\"  Non-zero min: {id_lvl_feats_labeled['non_zero_min'].sum()}\")\n",
    "\n",
    "print(\"\\nLie detector flags:\")\n",
    "print(f\"  Suspect (1+ detectors): {id_lvl_feats_labeled['suspect'].sum()}\")\n",
    "print(f\"  Highly suspect (2+ detectors): {id_lvl_feats_labeled['highly_suspect'].sum()}\")\n",
    "\n",
    "print(\"\\nMost common flag patterns:\")\n",
    "print(id_lvl_feats_labeled[\"flag_pattern\"].value_counts().head(10))\n",
    "\n",
    "# View highly suspect series with all context\n",
    "print(\"\\nHighly suspect series with characteristics:\")\n",
    "display(\n",
    "    id_lvl_feats_labeled[id_lvl_feats_labeled[\"highly_suspect\"]][\n",
    "        [\n",
    "            \"unique_id\",\n",
    "            \"intermittent\",\n",
    "            \"heavy_tailed\",\n",
    "            \"non_zero_min\",\n",
    "            \"n_flags\",\n",
    "            \"flagged_detectors\",\n",
    "            \"adi\",\n",
    "            \"kurtosis\",\n",
    "        ]\n",
    "    ].head(10)\n",
    ")\n",
    "\n",
    "# Crosstab: how do characteristics relate to detector flags?\n",
    "print(\"\\nIntermittent series that are also highly suspect:\")\n",
    "print(pd.crosstab(id_lvl_feats_labeled[\"intermittent\"], id_lvl_feats_labeled[\"highly_suspect\"]))\n",
    "\n",
    "print(\"\\nHeavy-tailed series that are also highly suspect:\")\n",
    "print(pd.crosstab(id_lvl_feats_labeled[\"heavy_tailed\"], id_lvl_feats_labeled[\"highly_suspect\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at items that are in the top25th percentile in our diagnostics\n",
    "    - this will help us spot certain more nuanced patterns\n",
    "    - Are intermittent items lumpy? Are seasonal items intermittent? Etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characteristic flags:\n",
      "  Intermittent: 11924\n",
      "  Heavy-tailed: 6394\n",
      "  Non-zero min: 752\n",
      "\n",
      "Lie detector flags:\n",
      "  Suspect (1+ detectors): 24698\n",
      "  Highly suspect (2+ detectors): 14898\n",
      "\n",
      "Most common flag patterns:\n",
      "flag_pattern\n",
      "CLEAN        5792\n",
      "MI_T|ADI     3199\n",
      "PERM         2793\n",
      "LUMP|PERM    1955\n",
      "LUMP         1870\n",
      "MI_T         1735\n",
      "ADI          1389\n",
      "SEAS|TREN    1328\n",
      "SEAS         1197\n",
      "LUMP|TREN     975\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Highly suspect series with characteristics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>intermittent</th>\n",
       "      <th>heavy_tailed</th>\n",
       "      <th>non_zero_min</th>\n",
       "      <th>n_flags</th>\n",
       "      <th>flagged_detectors</th>\n",
       "      <th>adi</th>\n",
       "      <th>kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FOODS_1_002_TX_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[MI_top_k_lags, adi]</td>\n",
       "      <td>1.612717</td>\n",
       "      <td>4.299198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FOODS_1_003_CA_4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[permutation_entropy, MI_top_k_lags]</td>\n",
       "      <td>1.097656</td>\n",
       "      <td>15.750452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>FOODS_1_003_TX_3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[MI_top_k_lags, adi]</td>\n",
       "      <td>1.694611</td>\n",
       "      <td>1.766621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>FOODS_1_004_CA_2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>[lumpiness, permutation_entropy, trend]</td>\n",
       "      <td>1.097561</td>\n",
       "      <td>3.194076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>FOODS_1_004_TX_2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[lumpiness, trend]</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>0.837187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>FOODS_1_004_WI_2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>[lumpiness, permutation_entropy, MI_top_k_lags]</td>\n",
       "      <td>1.092233</td>\n",
       "      <td>2.144132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>FOODS_1_004_WI_3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[lumpiness, permutation_entropy]</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.918513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>FOODS_1_005_CA_1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[lumpiness, seasonal_strength]</td>\n",
       "      <td>1.387255</td>\n",
       "      <td>1.836866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>FOODS_1_005_CA_3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[lumpiness, permutation_entropy]</td>\n",
       "      <td>1.214592</td>\n",
       "      <td>1.092073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>FOODS_1_006_CA_3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>[lumpiness, permutation_entropy]</td>\n",
       "      <td>1.064394</td>\n",
       "      <td>3.127723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           unique_id  intermittent  heavy_tailed  non_zero_min  n_flags  \\\n",
       "14  FOODS_1_002_TX_1          True          True         False        2   \n",
       "23  FOODS_1_003_CA_4         False          True         False        2   \n",
       "26  FOODS_1_003_TX_3          True         False         False        2   \n",
       "31  FOODS_1_004_CA_2         False          True         False        3   \n",
       "35  FOODS_1_004_TX_2         False         False         False        2   \n",
       "38  FOODS_1_004_WI_2         False         False         False        3   \n",
       "39  FOODS_1_004_WI_3         False         False         False        2   \n",
       "40  FOODS_1_005_CA_1          True         False         False        2   \n",
       "42  FOODS_1_005_CA_3         False         False         False        2   \n",
       "52  FOODS_1_006_CA_3         False          True         False        2   \n",
       "\n",
       "                                  flagged_detectors       adi   kurtosis  \n",
       "14                             [MI_top_k_lags, adi]  1.612717   4.299198  \n",
       "23             [permutation_entropy, MI_top_k_lags]  1.097656  15.750452  \n",
       "26                             [MI_top_k_lags, adi]  1.694611   1.766621  \n",
       "31          [lumpiness, permutation_entropy, trend]  1.097561   3.194076  \n",
       "35                               [lumpiness, trend]  1.125000   0.837187  \n",
       "38  [lumpiness, permutation_entropy, MI_top_k_lags]  1.092233   2.144132  \n",
       "39                 [lumpiness, permutation_entropy]  1.125000   1.918513  \n",
       "40                   [lumpiness, seasonal_strength]  1.387255   1.836866  \n",
       "42                 [lumpiness, permutation_entropy]  1.214592   1.092073  \n",
       "52                 [lumpiness, permutation_entropy]  1.064394   3.127723  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intermittent series that are also highly suspect:\n",
      "highly_suspect  False  True \n",
      "intermittent                \n",
      "False           10942   7624\n",
      "True             4650   7274\n",
      "\n",
      "Heavy-tailed series that are also highly suspect:\n",
      "highly_suspect  False  True \n",
      "heavy_tailed                \n",
      "False           13147  10949\n",
      "True             2445   3949\n"
     ]
    }
   ],
   "source": [
    "# Create individual detector flags\n",
    "\n",
    "id_lvl_feats[\"linearity_abs\"] = id_lvl_feats[\"linearity\"].abs()\n",
    "\n",
    "\n",
    "for detector in ld_six + ['linearity_abs']:\n",
    "    id_lvl_feats[f\"{detector}_flag\"] = id_lvl_feats[detector] > id_lvl_feats[detector].quantile(\n",
    "        0.75\n",
    "    )\n",
    "\n",
    "# Build labeled dataset with prominent flags + detector details\n",
    "flag_cols = [f\"{d}_flag\" for d in ld_six]\n",
    "\n",
    "id_lvl_feats_labeled = id_lvl_feats.assign(\n",
    "    # Prominent characteristic flags\n",
    "    intermittent=id_lvl_feats[\"adi\"] >= 1.34,\n",
    "    heavy_tailed=id_lvl_feats[\"kurtosis\"].abs() > 3, # look at heavytailed behavior\n",
    "    non_zero_min=id_lvl_feats[\"min\"] > 0,\n",
    "    # Detector summary flags\n",
    "    n_flags=id_lvl_feats[flag_cols].sum(axis=1),\n",
    "    suspect=id_lvl_feats[flag_cols].any(axis=1),\n",
    "    highly_suspect=lambda df: df[\"n_flags\"] >= 2,\n",
    "    # Which detectors are flagging\n",
    "    flagged_detectors=id_lvl_feats[flag_cols].apply(\n",
    "        lambda row: [ld_six[i] for i, val in enumerate(row) if val], axis=1\n",
    "    ),\n",
    "    # Compact string representation\n",
    "    flag_pattern=lambda df: df[\"flagged_detectors\"].apply(\n",
    "        lambda x: \"|\".join([d[:4].upper() for d in x]) if x else \"CLEAN\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Quick summary\n",
    "print(\"Characteristic flags:\")\n",
    "print(f\"  Intermittent: {id_lvl_feats_labeled['intermittent'].sum()}\")\n",
    "print(f\"  Heavy-tailed: {id_lvl_feats_labeled['heavy_tailed'].sum()}\")\n",
    "print(f\"  Non-zero min: {id_lvl_feats_labeled['non_zero_min'].sum()}\")\n",
    "\n",
    "print(\"\\nLie detector flags:\")\n",
    "print(f\"  Suspect (1+ detectors): {id_lvl_feats_labeled['suspect'].sum()}\")\n",
    "print(f\"  Highly suspect (2+ detectors): {id_lvl_feats_labeled['highly_suspect'].sum()}\")\n",
    "\n",
    "print(\"\\nMost common flag patterns:\")\n",
    "print(id_lvl_feats_labeled[\"flag_pattern\"].value_counts().head(10))\n",
    "\n",
    "# View highly suspect series with all context\n",
    "print(\"\\nHighly suspect series with characteristics:\")\n",
    "display(\n",
    "    id_lvl_feats_labeled[id_lvl_feats_labeled[\"highly_suspect\"]][\n",
    "        [\n",
    "            \"unique_id\",\n",
    "            \"intermittent\",\n",
    "            \"heavy_tailed\",\n",
    "            \"non_zero_min\",\n",
    "            \"n_flags\",\n",
    "            \"flagged_detectors\",\n",
    "            \"adi\",\n",
    "            \"kurtosis\",\n",
    "        ]\n",
    "    ].head(10)\n",
    ")\n",
    "\n",
    "# Crosstab: how do characteristics relate to detector flags?\n",
    "print(\"\\nIntermittent series that are also highly suspect:\")\n",
    "print(pd.crosstab(id_lvl_feats_labeled[\"intermittent\"], id_lvl_feats_labeled[\"highly_suspect\"]))\n",
    "\n",
    "print(\"\\nHeavy-tailed series that are also highly suspect:\")\n",
    "print(pd.crosstab(id_lvl_feats_labeled[\"heavy_tailed\"], id_lvl_feats_labeled[\"highly_suspect\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71391785"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmd_by_pattern = id_lvl_feats_labeled.groupby('flag_pattern')['pct_of_demand'].sum().sort_values(ascending=False)\n",
    "\n",
    "dmd_by_pattern.filter(like=\"LUMP\").sum() # 70% of our data has high lumpiness! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this is a big clue, this means we likely will need to use robust loss functions in any ML or DL approaches as the variance is highly unstable in many of our timeseries. \n",
    "\n",
    "* this also indicates some clear stationarity issues, we can see that the variance and likely the mean is not stable over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Demand with Lumpiness and Trend: 10.75%\n",
      "Percentage of Demand with Lumpiness and Seasonality: 3.53%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of Demand with Lumpiness and Trend: {dmd_by_pattern.filter(like=\"LUMP|TREN\").sum() * 100:.2f}%')\n",
    "print(f'Percentage of Demand with Lumpiness and Seasonality: {dmd_by_pattern.filter(like=\"LUMP|SEAS\").sum() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m id_lvl_feats.to_parquet(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mOUTPUT_DIR\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33muid_lvl_feats.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'OUTPUT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "id_lvl_feats.to_parquet(\n",
    "    OUTPUT_DIR / \"uid_lvl_feats.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Real World Forecasting (Poetry)",
   "language": "python",
   "name": "real-world-forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
